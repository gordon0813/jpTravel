{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  __[blog](https://blog.gtwang.org/programming/python-beautiful-soup-module-scrape-web-pages-tutorial/)__ #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qp=\"https://www.ptt.cc/bbs/e-shopping/search?page=2&q=%E6%B7%98%E5%AF%B6\"\n",
    "qp0='https://www.ptt.cc/bbs/Japan_Travel/search?page='\n",
    "qp1=\"&q=%E9%81%8A%E8%A8%98\"\n",
    "\n",
    "r=requests.get(qp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getword(ps):\n",
    "    path=\"https://www.ptt.cc\"+ps\n",
    "    r=requests.get(path)\n",
    "    soup = bs(r.text, 'html.parser')\n",
    "    return soup.get_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "textlist=[]\n",
    "author_ids = []\n",
    "for i in range(50):\n",
    "    r=requests.get(qp0+str(i)+qp1)\n",
    "    soup = bs(r.text, \"lxml\")\n",
    "    posts = soup.find_all(\"div\", class_ = \"r-ent\")\n",
    "    print(\"#############\",i,\"#####################\")\n",
    "    for post in posts:\n",
    "    \n",
    "        author_ids.extend(post.find(\"div\", class_ = \"author\"))\n",
    "        post2=post.find(\"div\", class_ = \"title\")\n",
    "        try:\n",
    "            a=post2.find(\"a\",href=True)[\"href\"]\n",
    "            print(a)\n",
    "            textlist.append(getword(a))\n",
    "        except:\n",
    "            print(\"No element\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textlist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newlist=[]\n",
    "for i in textlist:\n",
    "    newlist.append(i[i.find(\"看板Japan_Travel標題\"):i.rfind(\"(function\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newlist[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:/data/final_project/文章 new/食記.txt\",\"w\",encoding=\"utf8\") as file:\n",
    "    s5=\"split\".join(newlist)\n",
    "    file.write(s5)\n",
    "    \n",
    "a=\"123456\"\n",
    "a[1:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newlist[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(r.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = soup.find_all(\"div\", class_ = \"r-ent\")\n",
    "author_ids = []\n",
    "for post in posts:\n",
    "    \n",
    "    author_ids.extend(post.find(\"div\", class_ = \"author\"))\n",
    "    post2=post.find(\"div\", class_ = \"title\")\n",
    "    \n",
    "    a=post2.find(\"a\")\n",
    "    print(a)\n",
    "print(author_ids)\n",
    "\n",
    "print(type(str(author_ids[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in posts:\n",
    "    \n",
    "    author_ids.extend(post.find(\"div\", class_ = \"author\"))\n",
    "    post2=post.find(\"div\", class_ = \"title\")\n",
    "    try:\n",
    "        a=post2.find(\"a\",href=True)[\"href\"]\n",
    "        print(a)\n",
    "    except:\n",
    "        print(\"No element\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = \"\"\"\n",
    "<html><head><title>Hello World</title></head>\n",
    "<body><h2>Test Header</h2>\n",
    "<p>This is a test.</p>\n",
    "<a id=\"link1\" href=\"/my_link1\">Link 1</a>\n",
    "<a id=\"link2\" href=\"/my_link2\">Link 2</a>\n",
    "<p>Hello, <b class=\"boldtext\">Bold Text</b></p>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "print(soup)\n",
    "print(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = soup.find_all([\"a\", \"b\"])\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags1=soup.find_all([\"a\"])\n",
    "tags1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link2_tag = soup.find([\"a\"],id='link2')\n",
    "link2_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(link2_tag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(link2_tag.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=requests.get(\"https://acg.gamer.com.tw/acgDetail.php?s=56768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select(\"div .ACG-score\")[0].select(\"span\")[0].text#class-> .classA  #id-> #id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link3_tag = soup.find_all([\"a\"])\n",
    "for i in link3_tag:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_info_from_google(qu,page):\n",
    "    google_url = 'https://www.google.com.tw/search'\n",
    "    url_list=[]\n",
    "    title_list=[]\n",
    "    for i in range(page):\n",
    "        i=i*10\n",
    "        my_params = {'q': str(qu),\"start\":str(i)}\n",
    "        r = requests.get(google_url, params = my_params)\n",
    "        if (r.status_code == requests.codes.ok):\n",
    "            print(\"ok page=\",page)\n",
    "            soup =BeautifulSoup(r.text, 'html.parser')\n",
    "            items = soup.select(\"div .r a\")\n",
    "            for i in items:\n",
    "                text=i.text\n",
    "                herf=i.get('href')\n",
    "                print(\"標題：\" + text)\n",
    "                print(\"網址：\" + i.get('href'))\n",
    "                if(herf.find(\"http\")!=-1):\n",
    "                    herf=herf[herf.find(\"q=\")+2:herf.find(\"&sa\")]\n",
    "                    url_list.append(herf)\n",
    "                    title_list.append(text)\n",
    "    return (url_list,title_list)\n",
    "\n",
    "a=get_info_from_google(2018,1)\n",
    "print(a[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Google 搜尋 URL\n",
    "google_url = 'https://www.google.com.tw/search'\n",
    "\n",
    "# 查詢參數\n",
    "my_params = {'q': '2018'}\n",
    "\n",
    "# 下載 Google 搜尋結果\n",
    "r = requests.get(google_url, params = my_params)\n",
    "\n",
    "# 確認是否下載成功\n",
    "if r.status_code == requests.codes.ok:\n",
    "  # 以 BeautifulSoup 解析 HTML 原始碼\n",
    "  soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "  # 觀察 HTML 原始碼\n",
    "  # print(soup.prettify())\n",
    "\n",
    "  # 以 CSS 的選擇器來抓取 Google 的搜尋結果\n",
    "  items = soup.select(\"div .r a\")\n",
    "  for i in items:\n",
    "    # 標題\n",
    "    print(i)\n",
    "    print(\"標題：\" + i.text)\n",
    "    # 網址\n",
    "    print(\"網址：\" + i.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  \n",
    "\n",
    "html = \"\"\"  \n",
    "<html><head><title>The Dormouse's story</title></head>  \n",
    "<body>  \n",
    "<d class=\"title\" name=\"dromouse\"><a>The Dormouse's story</a></d>  \n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were  \n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and  \n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;  \n",
    "and they lived at the bottom of a well.</p>  \n",
    "</body>  \n",
    "</html>  \n",
    "\"\"\"  \n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')  \n",
    "print( soup.select('p')[0])\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[blog](http://www.bugingcode.com/blog/beautiful_soup_select.html)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select('body p a')) #一層一層  body->p->a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# useful   \n",
    "__[藤原栗子工作室](https://martychen920.blogspot.com/2017/05/python-crawler-beautifulsoup.html)__\n",
    "__[blog](http://blog.castman.net/%E6%95%99%E5%AD%B8/2016/12/22/python-data-science-tutorial-3.html)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "USER_AGENT = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'}\n",
    " \n",
    "def fetch_results(search_term, number_results, language_code):\n",
    "    \n",
    "    \n",
    "    assert isinstance(search_term, str), 'Search term must be a string'\n",
    "    assert isinstance(number_results, int), 'Number of results must be an integer'\n",
    "    escaped_search_term = search_term.replace(' ', '+')\n",
    "    google_url = 'https://www.google.com/search?q={}&num={}&hl={}'.format(escaped_search_term, number_results, language_code)\n",
    "    r = requests.get(google_url, headers=USER_AGENT)     \n",
    "  # 以 BeautifulSoup 解析 HTML 原始碼\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "  # 觀察 HTML 原始碼\n",
    "  # print(soup.prettify())\n",
    "\n",
    "  # 以 CSS 的選擇器來抓取 Google 的搜尋結果\n",
    "    items = soup.select('div.g > h3.r > a[href^=\"/url\"]')\n",
    "    for i in items:\n",
    "    # 標題\n",
    "        print(\"22222\")\n",
    "        print(\"標題：\" + i.text)\n",
    "    # 網址\n",
    "        print(\"網址：\" + i.get('href'))\n",
    "if __name__ == '__main__':\n",
    "    fetch_results('edmund martin', 100, 'en')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def google_scrape(Search_list):\n",
    "    title_list=[]\n",
    "    #url='http://www.baidu.com/s?rsv_idx=1&wd='LPL&usm=2&ie=utf-8&sl_lang=en&rsv_srlang=en&rsv_rq=en&rqlang=cn\n",
    "    url='https://www.google.com.tw/search?q='\n",
    "    user_agents = ['Mozilla/5.0 (Windows NT 6.1; WOW64; rv:23.0) Gecko/20130406 Firefox/23.0', \\\n",
    "          'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:18.0) Gecko/20100101 Firefox/18.0', \\\n",
    "          'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/533+ \\\n",
    "          (KHTML, like Gecko) Element Browser 5.0', \\\n",
    "          'IBM WebExplorer /v0.94', 'Galaxy/1.0 [en] (Mac OS X 10.5.6; U; en)', \\\n",
    "          'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)', \\\n",
    "          'Opera/9.80 (Windows NT 6.0) Presto/2.12.388 Version/12.14', \\\n",
    "          'Mozilla/5.0 (iPad; CPU OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) \\\n",
    "           Version/6.0 Mobile/10A5355d Safari/8536.25', \\\n",
    "          'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "           Chrome/28.0.1468.0 Safari/537.36', \\\n",
    "          'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0; TheWorld)']   \n",
    "    proxies = {\n",
    "      \"http\": \"http://113.200.214.164:9999\"}\n",
    "    index = random.randint(0, 9)\n",
    "    user_agent = user_agents[index]\n",
    "    for x in Search_list:\n",
    "        time.sleep(1)\n",
    "        res=requests.get()\n",
    "        soup=BeautifulSoup(res.text, \"html.parser\")\n",
    "        search_text=soup.find_all(\"div\", class_=\"g\")\n",
    "        title_list=[result.find(\"a\").text for result in search_text]\n",
    "        print  title_list\n",
    "\n",
    "google_scrape(['LOL LMS'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
